{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vqiJstCWK7f",
        "outputId": "69568e13-cc16-45c6-cc85-0d3aa97bf999"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.8.1)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.3.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.20.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.3)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.29.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.43.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.4)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Collecting urllib3<2.4.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.36.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.2.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.4)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-1.3.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.3.0-py3-none-any.whl (23 kB)\n",
            "Downloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.38.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m140.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=2e7c5fb689b131ae5c1ad1a32d44b81e81d27194ffc9daa4319602afc7d96ffa\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, urllib3, pyproject_hooks, pybase64, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, build, posthog, opentelemetry-semantic-conventions, onnxruntime, opentelemetry-sdk, kubernetes, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: opentelemetry-proto\n",
            "    Found existing installation: opentelemetry-proto 1.37.0\n",
            "    Uninstalling opentelemetry-proto-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-proto-1.37.0\n",
            "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
            "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n",
            "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.37.0\n",
            "    Uninstalling opentelemetry-api-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.37.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.37.0\n",
            "    Uninstalling opentelemetry-sdk-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.37.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.19.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-5.0.0 build-1.3.0 chromadb-1.3.5 coloredlogs-15.0.1 durationpy-0.10 httptools-0.7.1 humanfriendly-10.0 kubernetes-34.1.0 mmh3-5.2.0 onnxruntime-1.23.2 opentelemetry-api-1.38.0 opentelemetry-exporter-otlp-proto-common-1.38.0 opentelemetry-exporter-otlp-proto-grpc-1.38.0 opentelemetry-proto-1.38.0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 posthog-5.4.0 pybase64-1.4.2 pypika-0.48.9 pyproject_hooks-1.2.0 urllib3-2.3.0 uvloop-0.22.1 watchfiles-1.1.1\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Install dependencies\n",
        "!pip install openai chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iev6jS1_WNGJ",
        "outputId": "565be654-d552-4bee-d2b3-af635a3112b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Loaded 8347 items from /content/rag_dataset_final.json\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Load the JSON data\n",
        "import json\n",
        "import os\n",
        "\n",
        "file_path = \"/content/rag_dataset_final.json\"\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        json_data = json.load(f)\n",
        "    print(f\"âœ… Loaded {len(json_data)} items from {file_path}\")\n",
        "else:\n",
        "    print(f\"âŒ File not found at {file_path}. Please check the path.\")\n",
        "    json_data = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpC5YLuVXC7r",
        "outputId": "56a4a3f9-bcdb-41a5-95fb-09c1ca53e1aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Fireworks Embedding Client Ready.\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Setup Fireworks Client & Embedding Function\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize Client\n",
        "# âš ï¸ Note: Using the key provided in your script\n",
        "client = OpenAI(\n",
        "    api_key=\"FIREWORKS_API_KEY\",\n",
        "    base_url=\"https://api.fireworks.ai/inference/v1\"\n",
        ")\n",
        "\n",
        "EMBEDDING_MODEL = \"accounts/fireworks/models/qwen3-embedding-8b\"\n",
        "\n",
        "def get_fireworks_embeddings(text_list):\n",
        "    \"\"\"\n",
        "    Generates embeddings using Fireworks API.\n",
        "    Accepts a list of strings and returns a list of vectors.\n",
        "    \"\"\"\n",
        "    # API usually handles newlines better if replaced, but optional depending on model\n",
        "    clean_texts = [t.replace(\"\\n\", \" \") for t in text_list]\n",
        "\n",
        "    response = client.embeddings.create(\n",
        "        model=EMBEDDING_MODEL,\n",
        "        input=clean_texts\n",
        "    )\n",
        "\n",
        "    # Extract embeddings ensuring order is preserved\n",
        "    return [data.embedding for data in response.data]\n",
        "\n",
        "print(\"âœ… Fireworks Embedding Client Ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PYa_yRHDsoP",
        "outputId": "38cc2968-e705-43b1-a15b-149b42c351a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.12.3\n"
          ]
        }
      ],
      "source": [
        "import pydantic\n",
        "print(pydantic.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKCVRNF0DsoU",
        "outputId": "35575598-e7e6-446e-9641-fa3aaf83dde5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python path: /usr/bin/python3\n",
            "Pydantic version: 2.12.3\n"
          ]
        }
      ],
      "source": [
        "import sys, pydantic\n",
        "print(\"Python path:\", sys.executable)\n",
        "print(\"Pydantic version:\", pydantic.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoY8cDtGZA1s",
        "outputId": "08ab7b8a-c415-41f5-80f3-476f23ebb190"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing data lists...\n",
            "âš¡ Processing 8347 documents via Fireworks API...\n",
            "   Processing batch 0 to 100...\n",
            "   Processing batch 100 to 200...\n",
            "   Processing batch 200 to 300...\n",
            "   Processing batch 300 to 400...\n",
            "   Processing batch 400 to 500...\n",
            "   Processing batch 500 to 600...\n",
            "   Processing batch 600 to 700...\n",
            "   Processing batch 700 to 800...\n",
            "   Processing batch 800 to 900...\n",
            "   Processing batch 900 to 1000...\n",
            "   Processing batch 1000 to 1100...\n",
            "   Processing batch 1100 to 1200...\n",
            "   Processing batch 1200 to 1300...\n",
            "   Processing batch 1300 to 1400...\n",
            "   Processing batch 1400 to 1500...\n",
            "   Processing batch 1500 to 1600...\n",
            "   Processing batch 1600 to 1700...\n",
            "   Processing batch 1700 to 1800...\n",
            "   Processing batch 1800 to 1900...\n",
            "   Processing batch 1900 to 2000...\n",
            "   Processing batch 2000 to 2100...\n",
            "   Processing batch 2100 to 2200...\n",
            "   Processing batch 2200 to 2300...\n",
            "   Processing batch 2300 to 2400...\n",
            "   Processing batch 2400 to 2500...\n",
            "   Processing batch 2500 to 2600...\n",
            "   Processing batch 2600 to 2700...\n",
            "   Processing batch 2700 to 2800...\n",
            "   Processing batch 2800 to 2900...\n",
            "   Processing batch 2900 to 3000...\n",
            "   Processing batch 3000 to 3100...\n",
            "   Processing batch 3100 to 3200...\n",
            "   Processing batch 3200 to 3300...\n",
            "   Processing batch 3300 to 3400...\n",
            "   Processing batch 3400 to 3500...\n",
            "   Processing batch 3500 to 3600...\n",
            "   Processing batch 3600 to 3700...\n",
            "   Processing batch 3700 to 3800...\n",
            "   Processing batch 3800 to 3900...\n",
            "   Processing batch 3900 to 4000...\n",
            "   Processing batch 4000 to 4100...\n",
            "   Processing batch 4100 to 4200...\n",
            "   Processing batch 4200 to 4300...\n",
            "   Processing batch 4300 to 4400...\n",
            "   Processing batch 4400 to 4500...\n",
            "   Processing batch 4500 to 4600...\n",
            "   Processing batch 4600 to 4700...\n",
            "   Processing batch 4700 to 4800...\n",
            "   Processing batch 4800 to 4900...\n",
            "   Processing batch 4900 to 5000...\n",
            "   Processing batch 5000 to 5100...\n",
            "   Processing batch 5100 to 5200...\n",
            "   Processing batch 5200 to 5300...\n",
            "   Processing batch 5300 to 5400...\n",
            "   Processing batch 5400 to 5500...\n",
            "   Processing batch 5500 to 5600...\n",
            "   Processing batch 5600 to 5700...\n",
            "   Processing batch 5700 to 5800...\n",
            "   Processing batch 5800 to 5900...\n",
            "   Processing batch 5900 to 6000...\n",
            "   Processing batch 6000 to 6100...\n",
            "   Processing batch 6100 to 6200...\n",
            "   Processing batch 6200 to 6300...\n",
            "   Processing batch 6300 to 6400...\n",
            "   Processing batch 6400 to 6500...\n",
            "   Processing batch 6500 to 6600...\n",
            "   Processing batch 6600 to 6700...\n",
            "   Processing batch 6700 to 6800...\n",
            "   Processing batch 6800 to 6900...\n",
            "   Processing batch 6900 to 7000...\n",
            "   Processing batch 7000 to 7100...\n",
            "   Processing batch 7100 to 7200...\n",
            "   Processing batch 7200 to 7300...\n",
            "   Processing batch 7300 to 7400...\n",
            "   Processing batch 7400 to 7500...\n",
            "   Processing batch 7500 to 7600...\n",
            "   Processing batch 7600 to 7700...\n",
            "   Processing batch 7700 to 7800...\n",
            "   Processing batch 7800 to 7900...\n",
            "   Processing batch 7900 to 8000...\n",
            "   Processing batch 8000 to 8100...\n",
            "   Processing batch 8100 to 8200...\n",
            "   Processing batch 8200 to 8300...\n",
            "   Processing batch 8300 to 8347...\n",
            "âœ… Vector store created successfully!\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Create Vector Store\n",
        "import chromadb\n",
        "import time\n",
        "\n",
        "# 1. Setup Database Client\n",
        "db_client = chromadb.PersistentClient(path=\"./my_arabic_db\")\n",
        "\n",
        "# 2. Reset Collection (Delete old one to ensure schema matches new model)\n",
        "try:\n",
        "    db_client.delete_collection(name=\"grammar_collection\")\n",
        "    print(\"ğŸ—‘ï¸ Old collection deleted.\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "collection = db_client.get_or_create_collection(name=\"grammar_collection\")\n",
        "\n",
        "# 3. Prepare Data\n",
        "documents = []\n",
        "ids = []\n",
        "metadatas = []\n",
        "\n",
        "if json_data:\n",
        "    print(\"Preparing data lists...\")\n",
        "    for i, item in enumerate(json_data):\n",
        "        q_text = item.get('question_without_diacritics', item.get('question_with_diacritics', ''))\n",
        "        a_text = item.get('answer_without_diacritics', item.get('answer_with_diacritics', ''))\n",
        "\n",
        "        text_content = f\"Question: {q_text}\\nAnswer: {a_text}\"\n",
        "\n",
        "        documents.append(text_content)\n",
        "        ids.append(str(item.get('id', i)))\n",
        "        metadatas.append({\"source\": \"rag_dataset\"})\n",
        "\n",
        "    # 4. Generate Embeddings & Upsert in Batches\n",
        "    # We use a smaller batch size (e.g., 50) for API stability\n",
        "    BATCH_SIZE = 100\n",
        "    total_docs = len(documents)\n",
        "\n",
        "    print(f\"âš¡ Processing {total_docs} documents via Fireworks API...\")\n",
        "\n",
        "    for i in range(0, total_docs, BATCH_SIZE):\n",
        "        end = min(i + BATCH_SIZE, total_docs)\n",
        "\n",
        "        batch_docs = documents[i:end]\n",
        "        batch_ids = ids[i:end]\n",
        "        batch_meta = metadatas[i:end]\n",
        "\n",
        "        print(f\"   Processing batch {i} to {end}...\")\n",
        "\n",
        "        try:\n",
        "            # Get Embeddings from API\n",
        "            batch_embeddings = get_fireworks_embeddings(batch_docs)\n",
        "\n",
        "            # Save to ChromaDB\n",
        "            collection.upsert(\n",
        "                documents=batch_docs,\n",
        "                embeddings=batch_embeddings,\n",
        "                ids=batch_ids,\n",
        "                metadatas=batch_meta\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error on batch {i}: {e}\")\n",
        "            # Optional: Sleep briefly if hitting rate limits\n",
        "            time.sleep(10)\n",
        "\n",
        "    print(\"âœ… Vector store created successfully!\")\n",
        "else:\n",
        "    print(\"âš ï¸ No data found to process.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "UOT4icJIZa5o",
        "outputId": "578946f5-f699-4539-94b8-5c9eaf18559f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'# Cell 5: Strict Chat System (Anti-Hallucination)\\nimport chromadb\\n\\n# 1. Connect to Database\\ndb_client = chromadb.PersistentClient(path=\"./my_arabic_db\")\\ncollection = db_client.get_collection(name=\"grammar_collection\")\\n\\n# 2. Retrieval Function with Similarity Check (same as before)\\ndef retrieve_strict_context(query, threshold=0.4):\\n    \"\"\"\\n    Retrieves context but returns None if the context isn\\'t relevant enough.\\n    Threshold: Lower is stricter (0.0 is exact match, 1.0 is everything).\\n    \"\"\"\\n    try:\\n        query_vec = get_fireworks_embeddings([query])\\n\\n        results = collection.query(\\n            query_embeddings=query_vec,\\n            n_results=3,\\n            include=[\"documents\", \"distances\"]  # We need distances to check relevance\\n        )\\n\\n        if results[\"documents\"] and results[\"documents\"][0]:\\n            # NOTE: We are currently not filtering by distance threshold here.\\n            # You can optionally check results[\"distances\"][0][0] against `threshold`\\n            # if you want stricter behavior.\\n\\n            # Combine the documents\\n            context_text = \"\\n---\\n\".join(results[\"documents\"][0])\\n            return context_text\\n\\n        return None\\n    except Exception as e:\\n        print(f\"Retrieval Error: {e}\")\\n        return None\\n\\n# ---- Simple Short-Term Memory ----\\nconversation_history = []  # list of {\"role\": \"user\"/\"assistant\", \"content\": str}\\n\\ndef add_message(role: str, content: str):\\n    \"\"\"Save a message to the conversation history.\"\"\"\\n    conversation_history.append({\"role\": role, \"content\": content})\\n\\ndef get_recent_history(max_turns: int = 6) -> str:\\n    \"\"\"\\n    Return the last N messages (user + assistant) as a formatted text block.\\n    This will be included in the prompt as chat history.\\n    \"\"\"\\n    recent_msgs = conversation_history[-max_turns:]\\n    lines = []\\n    for msg in recent_msgs:\\n        speaker = \"User\" if msg[\"role\"] == \"user\" else \"Assistant\"\\n        lines.append(f\"{speaker}: {msg[\\'content\\']}\")\\n    return \"\\n\".join(lines)\\n\\ndef reset_history():\\n    \"\"\"Call this when starting a new session / user (optional).\"\"\"\\n    conversation_history.clear()\\n# ----------------------------------\\n\\n# 3. Chat Loop\\nprint(\"ğŸ’¬ --- Strict Chat System Ready (Type \\'exit\\' to stop) ---\")\\n\\n# Strict System Instruction\\nsystem_instruction = \"\"\"\\nYou are a strict AI assistant. You must answer the user\\'s question strictly based ONLY on the provided Context.\\nRules:\\n0. Greetings & General Help: If the user says \"Hello\", \"Can you help?\", \"Thanks\", or generic phrases, reply politely and professionally in Arabic. You do NOT need context for this.\\n1. Do not use any outside knowledge or pre-trained information.\\n2. If the answer is found in the Context, answer in Arabic.\\n3. If the answer is NOT in the Context, you must strictly say: \"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„.\"\\n4. Do not make up facts.\\n\"\"\"\\n\\nwhile True:\\n    user_input = input(\"\\nğŸ‘¤ You: \")\\n    if user_input.lower() in [\"exit\", \"quit\"]:\\n        break\\n\\n    # 1) Ø§Ø­ÙØ¸ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙÙŠ Ø§Ù„ØªØ§Ø±ÙŠØ®\\n    add_message(\"user\", user_input)\\n\\n    # 2) Ø§Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ù† Ø§Ù„Ù€ RAG Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠ ÙÙ‚Ø·\\n    context = retrieve_strict_context(user_input)\\n\\n    # Ø¥Ø°Ø§ Ù…Ø§ ÙÙŠÙ‡ Ø³ÙŠØ§Ù‚ Ù…Ù†Ø§Ø³Ø¨ØŒ Ù†Ø·Ù„Ø¹ Ø¨Ø±Ø¯ Ø«Ø§Ø¨Øª\\n    if not context:\\n        no_context_reply = \"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£Ø¬Ø¯ Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ø·Ø§Ø¨Ù‚Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\"\\n        print(f\"ğŸ¤– AI: {no_context_reply}\")\\n        # Ù†Ø­ÙØ¸ Ø§Ù„Ø±Ø¯ ÙÙŠ Ø§Ù„Ù‡Ø³ØªÙˆØ±ÙŠ Ø¨Ø±Ø¶Ù‡ Ù„Ùˆ Ø­Ø§Ø¨ÙŠÙ†\\n        add_message(\"assistant\", no_context_reply)\\n        continue\\n\\n    # 3) Ø¬Ù‡Ù‘Ø² Ø§Ù„Ù€ history Ø§Ù„Ù†ØµÙŠ (Ø¢Ø®Ø± N Ø±Ø³Ø§Ø¦Ù„ ÙÙ‚Ø·)\\n    history_text = get_recent_history(max_turns=6)\\n\\n    # 4) Ù†Ø¨Ù†ÙŠ Ø§Ù„Ù€ prompt: history + context + Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠ\\n    prompt = f\"\"\"\\nConversation history (last turns):\\n{history_text}\\n\\nContext (retrieved from the knowledge base):\\n{context}\\n\\nCurrent user question:\\n{user_input}\\n\"\"\"\\n\\n    try:\\n        response = client.chat.completions.create(\\n            model=\"accounts/fireworks/models/qwen2p5-vl-32b-instruct\",\\n            messages=[\\n                {\"role\": \"system\", \"content\": system_instruction},\\n                {\"role\": \"user\", \"content\": prompt},\\n            ],\\n            # Temperature 0.0 = no creativity (good for strict RAG)\\n            temperature=0.0,\\n            max_tokens=200,\\n        )\\n\\n        answer = response.choices[0].message.content\\n        print(f\"ğŸ¤– AI: {answer}\")\\n\\n        # 5) Ø§Ø­ÙØ¸ Ø±Ø¯ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ ÙÙŠ Ø§Ù„ØªØ§Ø±ÙŠØ®\\n        add_message(\"assistant\", answer)\\n\\n    except Exception as e:\\n        print(f\"âŒ Error: {e}\")'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''# Cell 5: Strict Chat System (Anti-Hallucination)\n",
        "import chromadb\n",
        "\n",
        "# 1. Connect to Database\n",
        "db_client = chromadb.PersistentClient(path=\"./my_arabic_db\")\n",
        "collection = db_client.get_collection(name=\"grammar_collection\")\n",
        "\n",
        "# 2. Retrieval Function with Similarity Check (same as before)\n",
        "def retrieve_strict_context(query, threshold=0.4):\n",
        "    \"\"\"\n",
        "    Retrieves context but returns None if the context isn't relevant enough.\n",
        "    Threshold: Lower is stricter (0.0 is exact match, 1.0 is everything).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        query_vec = get_fireworks_embeddings([query])\n",
        "\n",
        "        results = collection.query(\n",
        "            query_embeddings=query_vec,\n",
        "            n_results=3,\n",
        "            include=[\"documents\", \"distances\"]  # We need distances to check relevance\n",
        "        )\n",
        "\n",
        "        if results[\"documents\"] and results[\"documents\"][0]:\n",
        "            # NOTE: We are currently not filtering by distance threshold here.\n",
        "            # You can optionally check results[\"distances\"][0][0] against `threshold`\n",
        "            # if you want stricter behavior.\n",
        "\n",
        "            # Combine the documents\n",
        "            context_text = \"\\n---\\n\".join(results[\"documents\"][0])\n",
        "            return context_text\n",
        "\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Retrieval Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# ---- Simple Short-Term Memory ----\n",
        "conversation_history = []  # list of {\"role\": \"user\"/\"assistant\", \"content\": str}\n",
        "\n",
        "def add_message(role: str, content: str):\n",
        "    \"\"\"Save a message to the conversation history.\"\"\"\n",
        "    conversation_history.append({\"role\": role, \"content\": content})\n",
        "\n",
        "def get_recent_history(max_turns: int = 6) -> str:\n",
        "    \"\"\"\n",
        "    Return the last N messages (user + assistant) as a formatted text block.\n",
        "    This will be included in the prompt as chat history.\n",
        "    \"\"\"\n",
        "    recent_msgs = conversation_history[-max_turns:]\n",
        "    lines = []\n",
        "    for msg in recent_msgs:\n",
        "        speaker = \"User\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
        "        lines.append(f\"{speaker}: {msg['content']}\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def reset_history():\n",
        "    \"\"\"Call this when starting a new session / user (optional).\"\"\"\n",
        "    conversation_history.clear()\n",
        "# ----------------------------------\n",
        "\n",
        "# 3. Chat Loop\n",
        "print(\"ğŸ’¬ --- Strict Chat System Ready (Type 'exit' to stop) ---\")\n",
        "\n",
        "# Strict System Instruction\n",
        "system_instruction = \"\"\"\n",
        "You are a strict AI assistant. You must answer the user's question strictly based ONLY on the provided Context.\n",
        "Rules:\n",
        "0. Greetings & General Help: If the user says \"Hello\", \"Can you help?\", \"Thanks\", or generic phrases, reply politely and professionally in Arabic. You do NOT need context for this.\n",
        "1. Do not use any outside knowledge or pre-trained information.\n",
        "2. If the answer is found in the Context, answer in Arabic.\n",
        "3. If the answer is NOT in the Context, you must strictly say: \"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„.\"\n",
        "4. Do not make up facts.\n",
        "\"\"\"\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"\\nğŸ‘¤ You: \")\n",
        "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "        break\n",
        "\n",
        "    # 1) Ø§Ø­ÙØ¸ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙÙŠ Ø§Ù„ØªØ§Ø±ÙŠØ®\n",
        "    add_message(\"user\", user_input)\n",
        "\n",
        "    # 2) Ø§Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ù† Ø§Ù„Ù€ RAG Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠ ÙÙ‚Ø·\n",
        "    context = retrieve_strict_context(user_input)\n",
        "\n",
        "    # Ø¥Ø°Ø§ Ù…Ø§ ÙÙŠÙ‡ Ø³ÙŠØ§Ù‚ Ù…Ù†Ø§Ø³Ø¨ØŒ Ù†Ø·Ù„Ø¹ Ø¨Ø±Ø¯ Ø«Ø§Ø¨Øª\n",
        "    if not context:\n",
        "        no_context_reply = \"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£Ø¬Ø¯ Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ø·Ø§Ø¨Ù‚Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\"\n",
        "        print(f\"ğŸ¤– AI: {no_context_reply}\")\n",
        "        # Ù†Ø­ÙØ¸ Ø§Ù„Ø±Ø¯ ÙÙŠ Ø§Ù„Ù‡Ø³ØªÙˆØ±ÙŠ Ø¨Ø±Ø¶Ù‡ Ù„Ùˆ Ø­Ø§Ø¨ÙŠÙ†\n",
        "        add_message(\"assistant\", no_context_reply)\n",
        "        continue\n",
        "\n",
        "    # 3) Ø¬Ù‡Ù‘Ø² Ø§Ù„Ù€ history Ø§Ù„Ù†ØµÙŠ (Ø¢Ø®Ø± N Ø±Ø³Ø§Ø¦Ù„ ÙÙ‚Ø·)\n",
        "    history_text = get_recent_history(max_turns=6)\n",
        "\n",
        "    # 4) Ù†Ø¨Ù†ÙŠ Ø§Ù„Ù€ prompt: history + context + Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠ\n",
        "    prompt = f\"\"\"\n",
        "Conversation history (last turns):\n",
        "{history_text}\n",
        "\n",
        "Context (retrieved from the knowledge base):\n",
        "{context}\n",
        "\n",
        "Current user question:\n",
        "{user_input}\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"accounts/fireworks/models/qwen2p5-vl-32b-instruct\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_instruction},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ],\n",
        "            # Temperature 0.0 = no creativity (good for strict RAG)\n",
        "            temperature=0.0,\n",
        "            max_tokens=200,\n",
        "        )\n",
        "\n",
        "        answer = response.choices[0].message.content\n",
        "        print(f\"ğŸ¤– AI: {answer}\")\n",
        "\n",
        "        # 5) Ø§Ø­ÙØ¸ Ø±Ø¯ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ ÙÙŠ Ø§Ù„ØªØ§Ø±ÙŠØ®\n",
        "        add_message(\"assistant\", answer)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error: {e}\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6ZsXGnnDso7",
        "outputId": "e9713220-0c8e-4e88-f4d5-7cc0f7c97b4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ’¬ Strict Chat System with Memory is loaded. Use chat_with_memory('Ø³Ø¤Ø§Ù„Ùƒ') to talk to the model.\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Strict Chat System (Anti-Hallucination + Short-Term Memory)\n",
        "import chromadb\n",
        "\n",
        "# 1. Connect to Database\n",
        "db_client = chromadb.PersistentClient(path=\"./my_arabic_db\")\n",
        "collection = db_client.get_collection(name=\"grammar_collection\")\n",
        "\n",
        "# 2. Retrieval Function with Similarity Check\n",
        "def retrieve_strict_context(query, threshold=0.4):\n",
        "    \"\"\"\n",
        "    Retrieves context but returns None if the context isn't relevant enough.\n",
        "    Threshold: Lower is stricter (0.0 is exact match, 1.0 is everything).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        query_vec = get_fireworks_embeddings([query])\n",
        "\n",
        "        results = collection.query(\n",
        "            query_embeddings=query_vec,\n",
        "            n_results=3,\n",
        "            include=[\"documents\", \"distances\"]  # We need distances to check relevance\n",
        "        )\n",
        "\n",
        "        if results[\"documents\"] and results[\"documents\"][0]:\n",
        "            # NOTE: We are currently not filtering by distance threshold here.\n",
        "            # You can optionally check results[\"distances\"][0][0] against `threshold`\n",
        "            # if you want stricter behavior.\n",
        "\n",
        "            # Combine the documents\n",
        "            context_text = \"\\n---\\n\".join(results[\"documents\"][0])\n",
        "            return context_text\n",
        "\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Retrieval Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# ---- Simple Short-Term Memory ----\n",
        "conversation_history = []  # list of {\"role\": \"user\"/\"assistant\", \"content\": str}\n",
        "\n",
        "def add_message(role: str, content: str):\n",
        "    \"\"\"Save a message to the conversation history.\"\"\"\n",
        "    conversation_history.append({\"role\": role, \"content\": content})\n",
        "\n",
        "def get_recent_history(max_turns: int = 6) -> str:\n",
        "    \"\"\"\n",
        "    Return the last N messages (user + assistant) as a formatted text block.\n",
        "    This will be included in the prompt as chat history.\n",
        "    \"\"\"\n",
        "    recent_msgs = conversation_history[-max_turns:]\n",
        "    lines = []\n",
        "    for msg in recent_msgs:\n",
        "        speaker = \"User\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
        "        lines.append(f\"{speaker}: {msg['content']}\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def reset_history():\n",
        "    \"\"\"Call this when starting a new session / user (optional).\"\"\"\n",
        "    conversation_history.clear()\n",
        "# ----------------------------------\n",
        "\n",
        "# 3. System Instruction\n",
        "system_instruction = \"\"\"\n",
        "You are a strict AI assistant. You must answer the user's question strictly based ONLY on the provided Context.\n",
        "Rules:\n",
        "0. Greetings & General Help: If the user says \"Hello\", \"Can you help?\", \"Thanks\", or generic phrases, reply politely and professionally in Arabic. You do NOT need context for this.\n",
        "1. Do not use any outside knowledge or pre-trained information.\n",
        "2. If the answer is found in the Context, answer in Arabic.\n",
        "3. If the answer is NOT in the Context, you must strictly say: \"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„.\"\n",
        "4. Do not make up facts.\n",
        "\"\"\"\n",
        "\n",
        "print(\"ğŸ’¬ Strict Chat System with Memory is loaded. Use chat_with_memory('Ø³Ø¤Ø§Ù„Ùƒ') to talk to the model.\")\n",
        "\n",
        "def chat_with_memory(user_input: str):\n",
        "    \"\"\"Send one turn to the RAG chatbot with short-term memory.\"\"\"\n",
        "    # 1) Ø§Ø­ÙØ¸ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙÙŠ Ø§Ù„ØªØ§Ø±ÙŠØ®\n",
        "    add_message(\"user\", user_input)\n",
        "\n",
        "    # 2) Ø§Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ù† Ø§Ù„Ù€ RAG Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠ ÙÙ‚Ø·\n",
        "    context = retrieve_strict_context(user_input)\n",
        "\n",
        "    # Ø¥Ø°Ø§ Ù…Ø§ ÙÙŠÙ‡ Ø³ÙŠØ§Ù‚ Ù…Ù†Ø§Ø³Ø¨ØŒ Ù†Ø·Ù„Ø¹ Ø¨Ø±Ø¯ Ø«Ø§Ø¨Øª\n",
        "    if not context:\n",
        "        no_context_reply = \"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£Ø¬Ø¯ Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ø·Ø§Ø¨Ù‚Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\"\n",
        "        print(f\"ğŸ¤– AI: {no_context_reply}\")\n",
        "        add_message(\"assistant\", no_context_reply)\n",
        "        return no_context_reply\n",
        "\n",
        "    # 3) Ø¬Ù‡Ù‘Ø² Ø§Ù„Ù€ history Ø§Ù„Ù†ØµÙŠ (Ø¢Ø®Ø± N Ø±Ø³Ø§Ø¦Ù„ ÙÙ‚Ø·)\n",
        "    history_text = get_recent_history(max_turns=6)\n",
        "\n",
        "    # 4) Ù†Ø¨Ù†ÙŠ Ø§Ù„Ù€ prompt: history + context + Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠ\n",
        "    prompt = f\"\"\"\n",
        "Conversation history (last turns):\n",
        "{history_text}\n",
        "\n",
        "Context (retrieved from the knowledge base):\n",
        "{context}\n",
        "\n",
        "Current user question:\n",
        "{user_input}\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"accounts/fireworks/models/qwen2p5-vl-32b-instruct\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_instruction},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ],\n",
        "            temperature=0.0,  # no creativity (strict RAG)\n",
        "            max_tokens=200,\n",
        "        )\n",
        "\n",
        "        answer = response.choices[0].message.content\n",
        "        print(f\"ğŸ¤– AI: {answer}\")\n",
        "\n",
        "        # 5) Ø§Ø­ÙØ¸ Ø±Ø¯ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ ÙÙŠ Ø§Ù„ØªØ§Ø±ÙŠØ®\n",
        "        add_message(\"assistant\", answer)\n",
        "        return answer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "iSp0YxPkDsqN",
        "outputId": "e094b527-96bd-4e1b-ad55-c71e4d2686ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¤– AI: Ø§Ù„Ø§ØµÙ„ Ø¹Ø¯Ù… Ù…Ø®Ø§Ù„ÙÙ‡ Ø§Ù„Ù‚ÙŠØ§Ø³.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Ø§Ù„Ø§ØµÙ„ Ø¹Ø¯Ù… Ù…Ø®Ø§Ù„ÙÙ‡ Ø§Ù„Ù‚ÙŠØ§Ø³.'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_with_memory('Ø§Ø°ÙƒØ± Ù‚Ø§Ø¹Ø¯Ù‡ Ù…Ù† Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù…Ù†Ù‡Ø¬ÙŠÙ‡ Ø§Ù„Ø¹Ø§Ù…Ù‡ ÙÙŠ Ø§Ù„Ù‚ÙŠØ§Ø³ Ø¹Ù†Ø¯ Ø§Ù„Ù†Ø­ÙˆÙŠÙŠÙ†ØŸ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "VpeAb75NDsqR",
        "outputId": "ed7f6270-cacd-4efa-92c3-3fe6a34c72c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¤– AI: Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„.'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_with_memory(\"ÙˆØ¶Ø­ Ø£ÙƒØ«Ø± Ø§Ù„Ù…Ø«Ø§Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMvYxt_FDsqT",
        "outputId": "843d2b4e-6cbc-4e0d-f7fa-0f0d2ef863be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Ø§Ø°ÙƒØ± Ù‚Ø§Ø¹Ø¯Ù‡ Ù…Ù† Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù…Ù†Ù‡Ø¬ÙŠÙ‡ Ø§Ù„Ø¹Ø§Ù…Ù‡ ÙÙŠ Ø§Ù„Ù‚ÙŠØ§Ø³ Ø¹Ù†Ø¯ Ø§Ù„Ù†Ø­ÙˆÙŠÙŠÙ†ØŸ\n",
            "Assistant: Ø§Ù„Ø§ØµÙ„ Ø¹Ø¯Ù… Ù…Ø®Ø§Ù„ÙÙ‡ Ø§Ù„Ù‚ÙŠØ§Ø³.\n",
            "User: ÙˆØ¶Ø­ Ø£ÙƒØ«Ø± Ø§Ù„Ù…Ø«Ø§Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚\n",
            "Assistant: Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„.\n"
          ]
        }
      ],
      "source": [
        "#Ø¹Ø´Ø§Ù† Ø§Ø´ÙˆÙ Ø§Ø°Ø§ ÙØ¹Ù„Ù‹Ø§ Ù…ØªØ³Ù„Ø³Ù„ ÙˆÙ„Ø§ Ù„Ø§\n",
        "print(get_recent_history())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "2RNgX3tCDsqY",
        "outputId": "93418201-d415-4f60-b625-57dfe2ba4111"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¤– AI: Ù†Ø¹Ù…ØŒ ØªÙˆØ¬Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ù…Ø®ØªÙ„ÙØ© Ø¹Ù† \"Ø§Ù„Ø£ØµÙ„ Ø¹Ø¯Ù… Ù…Ø®Ø§Ù„ÙØ© Ø§Ù„Ù‚ÙŠØ§Ø³\". Ù…Ù† Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù…Ù†Ù‡Ø¬ÙŠØ© Ø§Ù„Ø¹Ø§Ù…Ø© ÙÙŠ Ø§Ù„Ù‚ÙŠØ§Ø³ Ø¹Ù†Ø¯ Ø§Ù„Ù†Ø­ÙˆÙŠÙŠÙ†: \"Ù„ÙŠØ³ Ø´Ø±Ø·Ø§ ÙÙŠ Ø§Ù„Ù‚ÙŠØ§Ø³ Ù…Ø³Ø§ÙˆØ§Ø© Ø§Ù„Ù…Ù‚ÙŠØ³ ÙˆØ§Ù„Ù…Ù‚ÙŠØ³ Ø¹Ù„ÙŠÙ‡ ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø­ÙƒØ§Ù….\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Ù†Ø¹Ù…ØŒ ØªÙˆØ¬Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ù…Ø®ØªÙ„ÙØ© Ø¹Ù† \"Ø§Ù„Ø£ØµÙ„ Ø¹Ø¯Ù… Ù…Ø®Ø§Ù„ÙØ© Ø§Ù„Ù‚ÙŠØ§Ø³\". Ù…Ù† Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù…Ù†Ù‡Ø¬ÙŠØ© Ø§Ù„Ø¹Ø§Ù…Ø© ÙÙŠ Ø§Ù„Ù‚ÙŠØ§Ø³ Ø¹Ù†Ø¯ Ø§Ù„Ù†Ø­ÙˆÙŠÙŠÙ†: \"Ù„ÙŠØ³ Ø´Ø±Ø·Ø§ ÙÙŠ Ø§Ù„Ù‚ÙŠØ§Ø³ Ù…Ø³Ø§ÙˆØ§Ø© Ø§Ù„Ù…Ù‚ÙŠØ³ ÙˆØ§Ù„Ù…Ù‚ÙŠØ³ Ø¹Ù„ÙŠÙ‡ ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø­ÙƒØ§Ù….\"'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_with_memory(\"Ù‡Ù„ ØªÙˆØ¬Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ù…Ø®ØªÙ„ÙØ© Ø¹Ù† ( Ø§Ù„Ø£ØµÙ„ Ø¹Ø¯Ù… Ù…Ø®Ø§Ù„ÙØ© Ø§Ù„Ù‚ÙŠØ§Ø³ØŸ)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "anvgtzpqDsqd"
      },
      "outputs": [],
      "source": [
        "#ØªØµÙÙŠØ± Ù„Ù„Ù…ÙŠÙ…ÙˆØ±ÙŠ\n",
        "reset_history()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6yEUm-ipDsqe",
        "outputId": "4e18084a-042d-4b69-93d0-aa5417393141"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¤– AI: (Ø§Ù„ÙƒØªØ§Ø¨) ÙÙŠ Ø§Ù„Ø§ØµÙ„ Ù…ØµØ¯Ø± (ÙƒØªØ¨)ØŒ Ø«Ù… ØµØ§Ø± Ø§Ø³Ù…Ø§ Ù„Ù„Ù‚Ø±Ø§Ù† Ø§Ù„ÙƒØ±ÙŠÙ….\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'(Ø§Ù„ÙƒØªØ§Ø¨) ÙÙŠ Ø§Ù„Ø§ØµÙ„ Ù…ØµØ¯Ø± (ÙƒØªØ¨)ØŒ Ø«Ù… ØµØ§Ø± Ø§Ø³Ù…Ø§ Ù„Ù„Ù‚Ø±Ø§Ù† Ø§Ù„ÙƒØ±ÙŠÙ….'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_with_memory('Ù…Ø§ Ø§Ù„Ø§ØµÙ„ ÙÙŠ ÙƒÙ„Ù…Ù‡ (Ø§Ù„ÙƒØªØ§Ø¨)ØŸ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "cOIlGVeADsqh",
        "outputId": "f72911eb-b0bd-4c94-edec-c659f1159203"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¤– AI: Ø§Ù„ÙƒØªØ§Ø¨.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Ø§Ù„ÙƒØªØ§Ø¨.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_with_memory(\"Ù…Ø§Ù„Ø°ÙŠ Ø§ØµØ¨Ø­ Ø§Ø³Ù… Ù„Ù„Ù‚Ø±Ø§Ù† Ø§Ù„ÙƒØ±ÙŠÙ…ØŸ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2MribkwDsqk",
        "outputId": "f18768a0-633a-4d70-f6a9-93e8ce3e093d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Ù…Ø§ Ø§Ù„Ø§ØµÙ„ ÙÙŠ ÙƒÙ„Ù…Ù‡ (Ø§Ù„ÙƒØªØ§Ø¨)ØŸ\n",
            "Assistant: (Ø§Ù„ÙƒØªØ§Ø¨) ÙÙŠ Ø§Ù„Ø§ØµÙ„ Ù…ØµØ¯Ø± (ÙƒØªØ¨)ØŒ Ø«Ù… ØµØ§Ø± Ø§Ø³Ù…Ø§ Ù„Ù„Ù‚Ø±Ø§Ù† Ø§Ù„ÙƒØ±ÙŠÙ….\n",
            "User: Ù…Ø§Ù„Ø°ÙŠ Ø§ØµØ¨Ø­ Ø§Ø³Ù… Ù„Ù„Ù‚Ø±Ø§Ù† Ø§Ù„ÙƒØ±ÙŠÙ…ØŸ\n",
            "Assistant: Ø§Ù„ÙƒØªØ§Ø¨.\n"
          ]
        }
      ],
      "source": [
        "#Ø¹Ø´Ø§Ù† Ø§Ø´ÙˆÙ Ø§Ø°Ø§ ÙØ¹Ù„Ù‹Ø§ Ù…ØªØ³Ù„Ø³Ù„ ÙˆÙ„Ø§ Ù„Ø§\n",
        "print(get_recent_history())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "yJTRyA1KDsqn",
        "outputId": "8eea5e9c-caa9-4d67-d72e-45d481a37f77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¤– AI: Ø§Ù„Ù…Ø¹Ø±Ø¨ Ø§Ù„Ù…Ù†ØµØ±Ù Ù‡Ùˆ Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø°ÙŠ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠØªØºÙŠØ± Ø£Ø¹Ø±Ø§Ø¨Ù‡ ÙˆÙŠÙ‚Ø¨Ù„ Ø§Ù„ØªÙ†ÙˆÙŠÙ†ØŒ Ù…Ø«Ù„ \"Ø¹Ù„ÙŠ\". Ø£Ù…Ø§ Ø§Ù„Ù…Ø¹Ø±Ø¨ ØºÙŠØ± Ø§Ù„Ù…Ù†ØµØ±Ù ÙÙ‡Ùˆ Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø°ÙŠ ØªØªØºÙŠØ± Ø­Ø§Ù„ØªÙ‡ Ø§Ù„Ø§Ø¹Ø±Ø§Ø¨ÙŠØ© ÙˆÙ„ÙƒÙ†Ù‡ Ù„Ø§ ÙŠÙ‚Ø¨Ù„ Ø§Ù„ØªÙ†ÙˆÙŠÙ†ØŒ Ù…Ø«Ù„ \"Ø£Ø­Ù…Ø¯\" Ùˆ\"ÙØ§Ø·Ù…Ø©\".\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Ø§Ù„Ù…Ø¹Ø±Ø¨ Ø§Ù„Ù…Ù†ØµØ±Ù Ù‡Ùˆ Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø°ÙŠ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠØªØºÙŠØ± Ø£Ø¹Ø±Ø§Ø¨Ù‡ ÙˆÙŠÙ‚Ø¨Ù„ Ø§Ù„ØªÙ†ÙˆÙŠÙ†ØŒ Ù…Ø«Ù„ \"Ø¹Ù„ÙŠ\". Ø£Ù…Ø§ Ø§Ù„Ù…Ø¹Ø±Ø¨ ØºÙŠØ± Ø§Ù„Ù…Ù†ØµØ±Ù ÙÙ‡Ùˆ Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø°ÙŠ ØªØªØºÙŠØ± Ø­Ø§Ù„ØªÙ‡ Ø§Ù„Ø§Ø¹Ø±Ø§Ø¨ÙŠØ© ÙˆÙ„ÙƒÙ†Ù‡ Ù„Ø§ ÙŠÙ‚Ø¨Ù„ Ø§Ù„ØªÙ†ÙˆÙŠÙ†ØŒ Ù…Ø«Ù„ \"Ø£Ø­Ù…Ø¯\" Ùˆ\"ÙØ§Ø·Ù…Ø©\".'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_with_memory(\"Ù…Ø§ Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ù…Ø¹Ø±Ø¨ Ø§Ù„Ù…Ù†ØµØ±Ù ÙˆØ§Ù„Ù…Ø¹Ø±Ø¨ ØºÙŠØ± Ø§Ù„Ù…Ù†ØµØ±Ù.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "0W89yHDaDsrP",
        "outputId": "53686ccc-6e6f-4580-c7d3-e8ccfa1cf38a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¤– AI: Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:  \n",
            "Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø°ÙŠ ÙŠÙØ·Ù„Ø¨ ÙÙŠÙ‡ Ø§Ù„Ù…Ø«Ø§Ù„. Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…ØªÙˆÙØ±ØŒ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ù…Ø«Ø§Ù„ Ù…ØªØ¹Ù„Ù‚Ù‹Ø§ Ø¨Ø§Ù„Ø¹Ø±Ø¶ Ø£Ùˆ Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø£Ùˆ Ø§Ù„Ø¥Ø¶Ø§ÙØ© \"Ø§Ø°\" Ø¥Ù„Ù‰ Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„ÙØ¹Ù„ÙŠØ©. Ø¥Ø°Ø§ ÙƒÙ†Øª ØªÙ‚ØµØ¯ Ù…Ø«Ø§Ù„Ù‹Ø§ Ù…Ø¹ÙŠÙ†Ù‹Ø§ØŒ ÙŠØ±Ø¬Ù‰ ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ø³Ø¤Ø§Ù„.  \n",
            "\n",
            "Ø¥Ø°Ø§ ÙƒÙ†Øª ØªÙ‚ØµØ¯ Ù…Ø«Ø§Ù„Ù‹Ø§ Ø¹Ø§Ù…Ù‹Ø§ØŒ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ†:  \n",
            "\"Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø°ÙŠ ÙŠÙØ·Ù„Ø¨ ÙÙŠÙ‡ Ø§Ù„Ù…Ø«Ø§Ù„.\"  \n",
            "\n",
            "Ø¥Ø°Ø§ ÙƒÙ†Øª ØªÙ‚ØµØ¯ Ù…Ø«Ø§Ù„Ù‹Ø§ Ù…Ø­Ø¯Ø¯Ù‹Ø§ Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ ÙŠØ±Ø¬Ù‰ ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ø³Ø¤Ø§Ù„.  \n",
            "\n",
            "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:  \\nØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø°ÙŠ ÙŠÙØ·Ù„Ø¨ ÙÙŠÙ‡ Ø§Ù„Ù…Ø«Ø§Ù„. Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…ØªÙˆÙØ±ØŒ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ù…Ø«Ø§Ù„ Ù…ØªØ¹Ù„Ù‚Ù‹Ø§ Ø¨Ø§Ù„Ø¹Ø±Ø¶ Ø£Ùˆ Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø£Ùˆ Ø§Ù„Ø¥Ø¶Ø§ÙØ© \"Ø§Ø°\" Ø¥Ù„Ù‰ Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„ÙØ¹Ù„ÙŠØ©. Ø¥Ø°Ø§ ÙƒÙ†Øª ØªÙ‚ØµØ¯ Ù…Ø«Ø§Ù„Ù‹Ø§ Ù…Ø¹ÙŠÙ†Ù‹Ø§ØŒ ÙŠØ±Ø¬Ù‰ ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ø³Ø¤Ø§Ù„.  \\n\\nØ¥Ø°Ø§ ÙƒÙ†Øª ØªÙ‚ØµØ¯ Ù…Ø«Ø§Ù„Ù‹Ø§ Ø¹Ø§Ù…Ù‹Ø§ØŒ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ†:  \\n\"Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø°ÙŠ ÙŠÙØ·Ù„Ø¨ ÙÙŠÙ‡ Ø§Ù„Ù…Ø«Ø§Ù„.\"  \\n\\nØ¥Ø°Ø§ ÙƒÙ†Øª ØªÙ‚ØµØ¯ Ù…Ø«Ø§Ù„Ù‹Ø§ Ù…Ø­Ø¯Ø¯Ù‹Ø§ Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ ÙŠØ±Ø¬Ù‰ ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ø³Ø¤Ø§Ù„.  \\n\\nØ¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_with_memory(\"Ø§Ø°ÙƒØ± Ù…Ø«Ø§Ù„\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "tnFCO2IfDsrQ",
        "outputId": "bd81e1d9-2b36-4b7b-cfdc-b4d01d8e4e8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¤– AI: Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„.'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_with_memory(\"Ø§Ø­Ù…Ø¯ Ùˆ ÙØ§Ø·Ù…Ù‡ ØŒ Ù…Ø«Ø§Ù„ Ù„Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚ØŸ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZEsGZsBDsrQ",
        "outputId": "061b4d01-4856-4315-a094-12651952c8ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Ù…Ø§ Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ù…Ø¹Ø±Ø¨ Ø§Ù„Ù…Ù†ØµØ±Ù ÙˆØ§Ù„Ù…Ø¹Ø±Ø¨ ØºÙŠØ± Ø§Ù„Ù…Ù†ØµØ±Ù.\n",
            "Assistant: Ø§Ù„Ù…Ø¹Ø±Ø¨ Ø§Ù„Ù…Ù†ØµØ±Ù Ù‡Ùˆ Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø°ÙŠ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠØªØºÙŠØ± Ø£Ø¹Ø±Ø§Ø¨Ù‡ ÙˆÙŠÙ‚Ø¨Ù„ Ø§Ù„ØªÙ†ÙˆÙŠÙ†ØŒ Ù…Ø«Ù„ \"Ø¹Ù„ÙŠ\". Ø£Ù…Ø§ Ø§Ù„Ù…Ø¹Ø±Ø¨ ØºÙŠØ± Ø§Ù„Ù…Ù†ØµØ±Ù ÙÙ‡Ùˆ Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø°ÙŠ ØªØªØºÙŠØ± Ø­Ø§Ù„ØªÙ‡ Ø§Ù„Ø§Ø¹Ø±Ø§Ø¨ÙŠØ© ÙˆÙ„ÙƒÙ†Ù‡ Ù„Ø§ ÙŠÙ‚Ø¨Ù„ Ø§Ù„ØªÙ†ÙˆÙŠÙ†ØŒ Ù…Ø«Ù„ \"Ø£Ø­Ù…Ø¯\" Ùˆ\"ÙØ§Ø·Ù…Ø©\".\n",
            "User: Ø§Ø°ÙƒØ± Ù…Ø«Ø§Ù„\n",
            "Assistant: Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:  \n",
            "Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø°ÙŠ ÙŠÙØ·Ù„Ø¨ ÙÙŠÙ‡ Ø§Ù„Ù…Ø«Ø§Ù„. Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…ØªÙˆÙØ±ØŒ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ù…Ø«Ø§Ù„ Ù…ØªØ¹Ù„Ù‚Ù‹Ø§ Ø¨Ø§Ù„Ø¹Ø±Ø¶ Ø£Ùˆ Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø£Ùˆ Ø§Ù„Ø¥Ø¶Ø§ÙØ© \"Ø§Ø°\" Ø¥Ù„Ù‰ Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„ÙØ¹Ù„ÙŠØ©. Ø¥Ø°Ø§ ÙƒÙ†Øª ØªÙ‚ØµØ¯ Ù…Ø«Ø§Ù„Ù‹Ø§ Ù…Ø¹ÙŠÙ†Ù‹Ø§ØŒ ÙŠØ±Ø¬Ù‰ ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ø³Ø¤Ø§Ù„.  \n",
            "\n",
            "Ø¥Ø°Ø§ ÙƒÙ†Øª ØªÙ‚ØµØ¯ Ù…Ø«Ø§Ù„Ù‹Ø§ Ø¹Ø§Ù…Ù‹Ø§ØŒ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ†:  \n",
            "\"Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø°ÙŠ ÙŠÙØ·Ù„Ø¨ ÙÙŠÙ‡ Ø§Ù„Ù…Ø«Ø§Ù„.\"  \n",
            "\n",
            "Ø¥Ø°Ø§ ÙƒÙ†Øª ØªÙ‚ØµØ¯ Ù…Ø«Ø§Ù„Ù‹Ø§ Ù…Ø­Ø¯Ø¯Ù‹Ø§ Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ ÙŠØ±Ø¬Ù‰ ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ø³Ø¤Ø§Ù„.  \n",
            "\n",
            "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„.\n",
            "User: Ø§Ø­Ù…Ø¯ Ùˆ ÙØ§Ø·Ù…Ù‡ ØŒ Ù…Ø«Ø§Ù„ Ù„Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚ØŸ\n",
            "Assistant: Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„.\n"
          ]
        }
      ],
      "source": [
        "#Ø¹Ø´Ø§Ù† Ø§Ø´ÙˆÙ Ø§Ø°Ø§ ÙØ¹Ù„Ù‹Ø§ Ù…ØªØ³Ù„Ø³Ù„ ÙˆÙ„Ø§ Ù„Ø§\n",
        "print(get_recent_history())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "P75KWuvKDsrV"
      },
      "outputs": [],
      "source": [
        "#ØªØµÙÙŠØ± Ù„Ù„Ù…ÙŠÙ…ÙˆØ±ÙŠ\n",
        "reset_history()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F218q2IxDsrV"
      },
      "source": [
        "Ø¨Ø¹Ø¯ Ø§Ù„Ø±Ù† Ù„Ù€ CEL 5\n",
        "\n",
        "New Cell:\n",
        "\n",
        "chat_with_memory(\"Ø§Ù„Ø³Ø¤Ø§Ù„ Ù‡Ù†Ø§\")\n",
        "\n",
        "Ø¹Ø´Ø§Ù† Ù†Ø´ÙˆÙ Ø¥Ø°Ø§ Ù…ØªØ³Ù„Ø³Ù„ ÙØ¹Ù„ÙŠÙ‹Ø§ Ø£Ùˆ Ù„Ø§ØŸ Ù†Ø³ÙˆÙŠ Print\n",
        "\n",
        "Ø£Ø®ÙŠØ±Ù‹Ø§ Ù†Ø³ÙˆÙŠ Reset\n",
        "\n",
        "reset_history()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "NOTE\n",
        "\n",
        "*Ø§Ù„Ù…ÙŠÙ…ÙˆØ±ÙŠ Ù‚ØµÙŠØ± Ø§Ù„Ù…Ø¯Ù‰ (Ø¢Ø®Ø± Ù¦ Ø±Ø³Ø§Ø¦Ù„ ØªÙ‚Ø±ÙŠØ¨Ù‹Ø§)ØŒ ÙˆÙ…Ø§ ÙÙŠÙ‡ ØªØ®Ø²ÙŠÙ† Ø¯Ø§Ø¦Ù… ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMp7cF_G0DU7",
        "outputId": "635d1560-399b-46f2-b6ca-9175cb152739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.37.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n",
            "Downloading groq-0.37.0-py3-none-any.whl (137 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/137.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.37.0\n"
          ]
        }
      ],
      "source": [
        "!pip install groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpKAuTxFEAcl",
        "outputId": "ef072f1a-9c7e-4d21-92fe-2368eb78ce98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Œ Evaluating 83 samples\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import sys\n",
        "import re\n",
        "import pandas as pd\n",
        "from groq import Groq\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ================== CONFIG ==================\n",
        "API_KEY = \"API Key\"  # better: use env var\n",
        "MODEL_NAME = \"llama-3.1-8b-instant\"\n",
        "DATASET_PATH = \"rag_dataset_final.json\"\n",
        "SAMPLE_PERCENT = 0.01  # 1% sample; increase carefully\n",
        "OUTPUT_FILE = \"evaluation_results.csv\"\n",
        "\n",
        "groq_client = Groq(api_key=API_KEY)\n",
        "\n",
        "# ================== LOAD DATA ==================\n",
        "with open(DATASET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "sample_size = max(1, int(len(data) * SAMPLE_PERCENT))\n",
        "dataset = random.sample(data, sample_size)\n",
        "total = len(dataset)\n",
        "\n",
        "print(f\"ğŸ“Œ Evaluating {total} samples\\n\")\n",
        "\n",
        "# ================== HELPERS ==================\n",
        "def extract_json(text: str) -> str:\n",
        "    \"\"\"Try to extract the first {...} JSON object from model output.\"\"\"\n",
        "    match = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
        "    return match.group(0) if match else text\n",
        "\n",
        "def simple_overlap(a: str, b: str) -> float:\n",
        "    \"\"\"Naive token overlap between two strings (works fine for Arabic too).\"\"\"\n",
        "    tokenize = lambda s: set(re.findall(r\"\\w+\", s.lower()))\n",
        "    ta, tb = tokenize(a), tokenize(b)\n",
        "    if not ta or not tb:\n",
        "        return 0.0\n",
        "    return len(ta & tb) / len(ta | tb)\n",
        "\n",
        "\n",
        "# ================== EVAL PROMPT ==================\n",
        "EVAL_PROMPT = \"\"\"\n",
        "You are evaluating a Retrieval-Augmented Generation (RAG) system.\n",
        "\n",
        "You are given:\n",
        "- A user question\n",
        "- Retrieved context from a knowledge base\n",
        "- The model's answer\n",
        "- The reference (ground truth) answer\n",
        "\n",
        "Score each metric from 0 to 3:\n",
        "\n",
        "- context_relevance: Are the retrieved contexts relevant to the question?\n",
        "- context_precision: How much of the retrieved text is actually useful?\n",
        "- answer_relevance: Does the model answer the user's question?\n",
        "- faithfulness: Is the model answer supported by the retrieved context only?\n",
        "- correctness: How close is the model answer to the reference answer?\n",
        "\n",
        "Return ONLY valid JSON with this exact structure:\n",
        "\n",
        "{{\n",
        "  \"context_relevance\": number,\n",
        "  \"context_precision\": number,\n",
        "  \"answer_relevance\": number,\n",
        "  \"faithfulness\": number,\n",
        "  \"correctness\": number,\n",
        "  \"comments\": \"short comment in Arabic about main issues\"\n",
        "}}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Reference Answer:\n",
        "{reference}\n",
        "\n",
        "Retrieved Context:\n",
        "{context}\n",
        "\n",
        "Model Answer:\n",
        "{answer}\n",
        "\"\"\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "xLd6URocFN7F"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ================== GROQ JUDGE ==================\n",
        "def evaluate_with_groq(question: str, reference: str, context, answer: str) -> dict:\n",
        "    \"\"\"Ask Groq model to score a single QA+context triple.\"\"\"\n",
        "    ctx_text = context if isinstance(context, str) else \"\\n\\n\".join(context)\n",
        "\n",
        "    prompt = EVAL_PROMPT.format(\n",
        "        question=question,\n",
        "        reference=reference,\n",
        "        context=ctx_text,\n",
        "        answer=answer,\n",
        "    )\n",
        "\n",
        "    completion = groq_client.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are an accurate RAG evaluation assistant. You MUST respond only with JSON, no extra text.\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        temperature=0,\n",
        "    )\n",
        "\n",
        "    response_text = completion.choices[0].message.content\n",
        "    response_text = extract_json(response_text)\n",
        "\n",
        "    try:\n",
        "        data = json.loads(response_text)\n",
        "        # ensure all keys exist\n",
        "        for k in [\n",
        "            \"context_relevance\",\n",
        "            \"context_precision\",\n",
        "            \"answer_relevance\",\n",
        "            \"faithfulness\",\n",
        "            \"correctness\",\n",
        "            \"comments\",\n",
        "        ]:\n",
        "            if k not in data:\n",
        "                if k == \"comments\":\n",
        "                    data[k] = \"\"\n",
        "                else:\n",
        "                    data[k] = 0\n",
        "        return data\n",
        "    except json.JSONDecodeError:\n",
        "        return {\n",
        "            \"context_relevance\": 0,\n",
        "            \"context_precision\": 0,\n",
        "            \"answer_relevance\": 0,\n",
        "            \"faithfulness\": 0,\n",
        "            \"correctness\": 0,\n",
        "            \"comments\": f\"âš  Could not parse judge output: {response_text}\",\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "r_iFEQ5qFU42"
      },
      "outputs": [],
      "source": [
        "# ================== SAFE WRAPPER WITH RATE-LIMIT HANDLING ==================\n",
        "rate_limit_count = 0\n",
        "\n",
        "def safe_rag_answer(q: str, max_retries: int = 5, base_delay: float = 1.0):\n",
        "    \"\"\"\n",
        "    Wrap rag_answer(q) with retry logic on Fireworks/Groq rate limits (429).\n",
        "    Fast normally; slows down only when rate limit happens.\n",
        "    \"\"\"\n",
        "    global rate_limit_count\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            x = chat_with_memory(q)\n",
        "            reset_history()\n",
        "            return x\n",
        "\n",
        "        except Exception as e:\n",
        "            msg = str(e).lower()\n",
        "\n",
        "            is_rate_limit = any(\n",
        "                key in msg\n",
        "                for key in [\n",
        "                    \"429\",\n",
        "                    \"rate limit\",\n",
        "                    \"rate_limit\",\n",
        "                    \"rate_limit_exceeded\",\n",
        "                    \"request rate limit exceeded\",\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            if not is_rate_limit:\n",
        "                # Non-rate-limit error -> let outer loop mark as skipped\n",
        "                print(f\"\\nâŒ Non rate-limit error in rag_answer:\")\n",
        "                print(\"   Error type:\", type(e).__name__)\n",
        "                print(\"   Error msg :\", e)\n",
        "                raise\n",
        "\n",
        "            rate_limit_count += 1\n",
        "            delay = base_delay * (2 ** attempt)  # 1s, 2s, 4s, ...\n",
        "            print(f\"\\nâš  Rate limit detected, backing off for {delay:.1f}s...\")\n",
        "\n",
        "    raise RuntimeError(\"Too many rate-limit failures for this question.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9A_10QM1p0W",
        "outputId": "582feed1-450a-434b-aa2e-1263f0ccec7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Œ Evaluating 417 samples\n",
            "\n",
            "ğŸ†• Starting fresh, no previous results found.\n",
            "Progress: 417/417 | Skipped: 0 | Rate-Limits: 0\n",
            "\n",
            "âœ… DONE\n",
            "ğŸ“ Saved â†’ results/eval_results.csv\n",
            "ğŸ“Š Evaluated this run: 417\n",
            "ğŸš« Skipped this run: 0\n",
            "âš  Rate-limit incidents: 0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import sys\n",
        "import re\n",
        "import pandas as pd\n",
        "from groq import Groq\n",
        "\n",
        "# ================== CONFIG ==================\n",
        "API_KEY = \"API Key\"\n",
        "MODEL_NAME = \"llama-3.1-8b-instant\"\n",
        "DATASET_PATH = \"rag_dataset_final.json\"\n",
        "SAMPLE_PERCENT = 0.05  # 1% sample\n",
        "OUTPUT_FILE = \"results/eval_results.csv\"\n",
        "\n",
        "groq_client = Groq(api_key=API_KEY)\n",
        "\n",
        "# ================== LOAD DATA ==================\n",
        "with open(DATASET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "sample_size = max(1, int(len(data) * SAMPLE_PERCENT))\n",
        "dataset = random.sample(data, sample_size)\n",
        "total = len(dataset)\n",
        "\n",
        "print(f\"ğŸ“Œ Evaluating {total} samples\\n\")\n",
        "\n",
        "# ================== HELPERS ==================\n",
        "def extract_json(text: str) -> str:\n",
        "    \"\"\"Try to extract the first {...} JSON object from model output.\"\"\"\n",
        "    match = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
        "    return match.group(0) if match else text\n",
        "\n",
        "def simple_overlap(a: str, b: str) -> float:\n",
        "    \"\"\"Naive token overlap between two strings.\"\"\"\n",
        "    tokenize = lambda s: set(re.findall(r\"\\w+\", s.lower()))\n",
        "    ta, tb = tokenize(a), tokenize(b)\n",
        "    if not ta or not tb:\n",
        "        return 0.0\n",
        "    return len(ta & tb) / len(ta | tb)\n",
        "\n",
        "# ================== EVAL PROMPT ==================\n",
        "EVAL_PROMPT_WITH_EXAMPLES = \"\"\"\n",
        "You are an expert evaluator for Retrieval-Augmented Generation (RAG) systems.\n",
        "\n",
        "=== EVALUATION METRICS (Scale 0-3) ===\n",
        "\n",
        "1. **Context Relevance**: Do contexts relate to the question?\n",
        "   - 3: Perfect match to question topic\n",
        "   - 2: Related with some relevant info\n",
        "   - 1: Tangentially related\n",
        "   - 0: Unrelated\n",
        "\n",
        "2. **Context Precision**: Useful info vs noise ratio?\n",
        "   - 3: >80% useful content\n",
        "   - 2: 50-80% useful content\n",
        "   - 1: 20-50% useful content\n",
        "   - 0: <20% useful content\n",
        "\n",
        "3. **Answer Relevance**: Does answer address question?\n",
        "   - 3: Directly and completely\n",
        "   - 2: Addresses but incomplete\n",
        "   - 1: Partially addresses\n",
        "   - 0: Doesn't address\n",
        "\n",
        "4. **Faithfulness**: Answer based on context only?\n",
        "   - 3: 100% from context\n",
        "   - 2: Mostly from context (>80%)\n",
        "   - 1: Some from context (50-80%)\n",
        "   - 0: Mostly hallucinated (<50%)\n",
        "\n",
        "5. **Correctness**: Match with reference answer?\n",
        "   - 3: Semantically equivalent\n",
        "   - 2: Core meaning correct\n",
        "   - 1: Partially correct\n",
        "   - 0: Wrong or opposite\n",
        "\n",
        "=== EXAMPLE ===\n",
        "\n",
        "Question: \"Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ø§Ø³Ù…ÙŠØ©ØŸ\"\n",
        "Context: \"Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ø§Ø³Ù…ÙŠØ© ØªØ¨Ø¯Ø£ Ø¨Ø§Ø³Ù… ÙˆØªØªÙƒÙˆÙ† Ù…Ù† Ù…Ø¨ØªØ¯Ø£ ÙˆØ®Ø¨Ø±\"\n",
        "Model: \"Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ø§Ø³Ù…ÙŠØ© ØªØ¨Ø¯Ø£ Ø¨Ø§Ø³Ù…\"\n",
        "Reference: \"Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ø§Ø³Ù…ÙŠØ© Ù‡ÙŠ Ø§Ù„ØªÙŠ ØªØ¨Ø¯Ø£ Ø¨Ø§Ø³Ù…\"\n",
        "\n",
        "Scores:\n",
        "- context_relevance: 3 (context perfectly matches question)\n",
        "- context_precision: 3 (all information is useful)\n",
        "- answer_relevance: 3 (directly answers the question)\n",
        "- faithfulness: 3 (answer is directly from context)\n",
        "- correctness: 2 (correct but less detailed than reference)\n",
        "\n",
        "=== YOUR TASK ===\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Reference Answer:\n",
        "{reference}\n",
        "\n",
        "Retrieved Context:\n",
        "{context}\n",
        "\n",
        "Model Answer:\n",
        "{answer}\n",
        "\n",
        "=== OUTPUT ===\n",
        "\n",
        "Return ONLY this JSON (no markdown, no extra text):\n",
        "\n",
        "{{\n",
        "  \"context_relevance\": <0-3>,\n",
        "  \"context_precision\": <0-3>,\n",
        "  \"answer_relevance\": <0-3>,\n",
        "  \"faithfulness\": <0-3>,\n",
        "  \"correctness\": <0-3>,\n",
        "  \"comments\": \"<Arabic comment>\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "# ================== GROQ JUDGE ==================\n",
        "def evaluate_with_groq(question: str, reference: str, context, answer: str) -> dict:\n",
        "    \"\"\"Ask Groq model to score a single QA+context triple.\"\"\"\n",
        "    ctx_text = context if isinstance(context, str) else \"\\n\\n\".join(context)\n",
        "\n",
        "    prompt = EVAL_PROMPT.format(\n",
        "        question=question,\n",
        "        reference=reference,\n",
        "        context=ctx_text,\n",
        "        answer=answer,\n",
        "    )\n",
        "\n",
        "    completion = groq_client.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are an accurate RAG evaluation assistant. You MUST respond only with JSON, no extra text.\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        temperature=0,\n",
        "    )\n",
        "\n",
        "    response_text = completion.choices[0].message.content\n",
        "    response_text = extract_json(response_text)\n",
        "\n",
        "    try:\n",
        "        data = json.loads(response_text)\n",
        "        # ensure all keys exist\n",
        "        for k in [\n",
        "            \"context_relevance\",\n",
        "            \"context_precision\",\n",
        "            \"answer_relevance\",\n",
        "            \"faithfulness\",\n",
        "            \"correctness\",\n",
        "            \"comments\",\n",
        "        ]:\n",
        "            if k not in data:\n",
        "                if k == \"comments\":\n",
        "                    data[k] = \"\"\n",
        "                else:\n",
        "                    data[k] = 0\n",
        "        return data\n",
        "    except json.JSONDecodeError:\n",
        "        return {\n",
        "            \"context_relevance\": 0,\n",
        "            \"context_precision\": 0,\n",
        "            \"answer_relevance\": 0,\n",
        "            \"faithfulness\": 0,\n",
        "            \"correctness\": 0,\n",
        "            \"comments\": f\"âš  Could not parse judge output: {response_text}\",\n",
        "        }\n",
        "\n",
        "# ================== MODIFIED RAG ANSWER WITH CONTEXT RETURN ==================\n",
        "def retrieve_strict_context(query, threshold=0.4):\n",
        "    \"\"\"\n",
        "    Retrieves context and returns both the context text and raw results.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        query_vec = get_fireworks_embeddings([query])\n",
        "\n",
        "        results = collection.query(\n",
        "            query_embeddings=query_vec,\n",
        "            n_results=3,\n",
        "            include=[\"documents\", \"distances\"]\n",
        "        )\n",
        "\n",
        "        if results[\"documents\"] and results[\"documents\"][0]:\n",
        "            context_text = \"\\n---\\n\".join(results[\"documents\"][0])\n",
        "            return context_text, results[\"documents\"][0]  # Return both combined and list\n",
        "\n",
        "        return None, []\n",
        "    except Exception as e:\n",
        "        print(f\"Retrieval Error: {e}\")\n",
        "        return None, []\n",
        "\n",
        "def chat_with_memory_and_context(user_input: str):\n",
        "    \"\"\"\n",
        "    Modified version that returns BOTH answer AND retrieved contexts.\n",
        "    \"\"\"\n",
        "    # 1) Save user message\n",
        "    add_message(\"user\", user_input)\n",
        "\n",
        "    # 2) Retrieve context\n",
        "    context, context_list = retrieve_strict_context(user_input)\n",
        "\n",
        "    # If no context found\n",
        "    if not context:\n",
        "        no_context_reply = \"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£Ø¬Ø¯ Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ø·Ø§Ø¨Ù‚Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\"\n",
        "        add_message(\"assistant\", no_context_reply)\n",
        "        return no_context_reply, []\n",
        "\n",
        "    # 3) Get conversation history\n",
        "    history_text = get_recent_history(max_turns=6)\n",
        "\n",
        "    # 4) Build prompt\n",
        "    prompt = f\"\"\"\n",
        "Conversation history (last turns):\n",
        "{history_text}\n",
        "\n",
        "Context (retrieved from the knowledge base):\n",
        "{context}\n",
        "\n",
        "Current user question:\n",
        "{user_input}\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"accounts/fireworks/models/qwen2p5-vl-32b-instruct\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_instruction},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ],\n",
        "            temperature=0.0,\n",
        "            max_tokens=200,\n",
        "        )\n",
        "\n",
        "        answer = response.choices[0].message.content\n",
        "\n",
        "        # 5) Save assistant response\n",
        "        add_message(\"assistant\", answer)\n",
        "\n",
        "        return answer, context_list  # Return both answer and contexts\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error: {e}\")\n",
        "        raise  # Re-raise to handle in safe_rag_answer\n",
        "\n",
        "# ================== SAFE WRAPPER WITH RATE-LIMIT HANDLING ==================\n",
        "rate_limit_count = 0\n",
        "\n",
        "def safe_rag_answer(q: str, max_retries: int = 5, base_delay: float = 1.0):\n",
        "    \"\"\"\n",
        "    Wrap RAG answer with retry logic on rate limits.\n",
        "    Returns (answer, contexts) tuple.\n",
        "    \"\"\"\n",
        "    global rate_limit_count\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            answer, contexts = chat_with_memory_and_context(q)\n",
        "            reset_history()\n",
        "            return answer, contexts\n",
        "\n",
        "        except Exception as e:\n",
        "            msg = str(e).lower()\n",
        "\n",
        "            is_rate_limit = any(\n",
        "                key in msg\n",
        "                for key in [\n",
        "                    \"429\",\n",
        "                    \"rate limit\",\n",
        "                    \"rate_limit\",\n",
        "                    \"rate_limit_exceeded\",\n",
        "                    \"request rate limit exceeded\",\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            if not is_rate_limit:\n",
        "                # Non-rate-limit error\n",
        "                print(f\"\\nâŒ Non rate-limit error in rag_answer:\")\n",
        "                print(\"   Error type:\", type(e).__name__)\n",
        "                print(\"   Error msg:\", e)\n",
        "                raise\n",
        "\n",
        "            rate_limit_count += 1\n",
        "            delay = base_delay * (2 ** attempt)\n",
        "            print(f\"\\nâš  Rate limit detected, backing off for {delay:.1f}s...\")\n",
        "            time.sleep(delay)\n",
        "\n",
        "    raise RuntimeError(\"Too many rate-limit failures for this question.\")\n",
        "\n",
        "# ================== MAIN EVAL LOOP ==================\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "results = []\n",
        "success_count = 0\n",
        "skipped_count = 0\n",
        "processed_ids = set()\n",
        "\n",
        "# Load previous results if exists\n",
        "if os.path.exists(OUTPUT_FILE):\n",
        "    prev_df = pd.read_csv(OUTPUT_FILE)\n",
        "    if \"id\" in prev_df.columns:\n",
        "        processed_ids = set(prev_df[\"id\"].tolist())\n",
        "    results = prev_df.to_dict(orient=\"records\")\n",
        "    success_count = len(results)\n",
        "    print(f\"ğŸ” Resuming: found {success_count} existing results in {OUTPUT_FILE}\")\n",
        "else:\n",
        "    print(\"ğŸ†• Starting fresh, no previous results found.\")\n",
        "\n",
        "def status_line():\n",
        "    sys.stdout.write(\n",
        "        f\"\\rProgress: {success_count}/{total} | \"\n",
        "        f\"Skipped: {skipped_count} | \"\n",
        "        f\"Rate-Limits: {rate_limit_count}\"\n",
        "    )\n",
        "    sys.stdout.flush()\n",
        "\n",
        "status_line()\n",
        "\n",
        "for i, row in enumerate(dataset):\n",
        "    # Skip already processed rows\n",
        "    if i in processed_ids:\n",
        "        continue\n",
        "\n",
        "    question = row[\"question_without_diacritics\"]\n",
        "    reference = row[\"answer_without_diacritics\"]\n",
        "\n",
        "    # 1) Retrieval + generation\n",
        "    try:\n",
        "        answer, contexts = safe_rag_answer(question)\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâš  Skipping sample {i} due to error: {e}\")\n",
        "        skipped_count += 1\n",
        "        status_line()\n",
        "        continue\n",
        "\n",
        "    # 2) Model-based evaluation\n",
        "    try:\n",
        "        score = evaluate_with_groq(question, reference, contexts, answer)\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâš  Evaluation failed for sample {i}: {e}\")\n",
        "        skipped_count += 1\n",
        "        status_line()\n",
        "        continue\n",
        "\n",
        "    # 3) Lexical heuristic\n",
        "    overlap = simple_overlap(answer, reference)\n",
        "\n",
        "    # Normalize contexts for logging\n",
        "    if isinstance(contexts, str):\n",
        "        ctx_joined = contexts\n",
        "    else:\n",
        "        ctx_joined = \" | \".join(contexts) if contexts else \"No context\"\n",
        "\n",
        "    # 4) Collect row\n",
        "    row_result = {\n",
        "        \"id\": i,\n",
        "        \"question\": question,\n",
        "        \"reference\": reference,\n",
        "        \"model_answer\": answer,\n",
        "        \"retrieved_contexts\": ctx_joined,\n",
        "        \"context_relevance\": score[\"context_relevance\"],\n",
        "        \"context_precision\": score[\"context_precision\"],\n",
        "        \"answer_relevance\": score[\"answer_relevance\"],\n",
        "        \"faithfulness\": score[\"faithfulness\"],\n",
        "        \"correctness\": score[\"correctness\"],\n",
        "        \"lexical_overlap\": round(overlap, 3),\n",
        "        \"comments\": score[\"comments\"],\n",
        "        \"final_score\": round(\n",
        "            (\n",
        "                score[\"context_relevance\"]\n",
        "                + score[\"context_precision\"]\n",
        "                + score[\"answer_relevance\"]\n",
        "                + score[\"faithfulness\"]\n",
        "                + score[\"correctness\"]\n",
        "            )\n",
        "            / 5,\n",
        "            3,\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    results.append(row_result)\n",
        "    success_count += 1\n",
        "\n",
        "    # 5) Save after each successful row\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(OUTPUT_FILE, index=False)\n",
        "\n",
        "    status_line()\n",
        "\n",
        "# ================== FINAL SUMMARY ==================\n",
        "print(\"\\n\\nâœ… DONE\")\n",
        "print(f\"ğŸ“ Saved â†’ {OUTPUT_FILE}\")\n",
        "print(f\"ğŸ“Š Evaluated this run: {success_count}\")\n",
        "print(f\"ğŸš« Skipped this run: {skipped_count}\")\n",
        "print(f\"âš  Rate-limit incidents: {rate_limit_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gj_vXky335TD",
        "outputId": "a1809d56-b85b-4759-b253-76f28b8d702c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading evaluation results...\n",
            "Loaded 417 samples\n",
            "\n",
            "Calculating retrieval metrics...\n",
            "Done!\n",
            "\n",
            "âœ… Saved results to: results/eval_results_with_retrieval_metrics.csv\n",
            "\n",
            "============================================================\n",
            "EVALUATION METRICS SUMMARY (Scale 0-1)\n",
            "============================================================\n",
            "\n",
            "Metric                         Mean            Std            \n",
            "------------------------------------------------------------\n",
            "context_relevance              0.990           0.090          \n",
            "context_precision              0.787           0.181          \n",
            "answer_relevance               0.980           0.132          \n",
            "faithfulness                   0.925           0.220          \n",
            "correctness                    0.923           0.224          \n",
            "lexical_overlap                0.723           0.327          \n",
            "\n",
            "============================================================\n",
            "RETRIEVAL METRICS SUMMARY (Scale 0-1)\n",
            "============================================================\n",
            "\n",
            "Metric                         Mean            Std            \n",
            "------------------------------------------------------------\n",
            "recall_at_3                    0.943           0.203          \n",
            "precision_at_3                 0.279           0.132          \n",
            "mrr                            0.925           0.244          \n",
            "ndcg_at_3                      0.939           0.203          \n",
            "\n",
            "============================================================\n",
            "INTERPRETATION & INSIGHTS\n",
            "============================================================\n",
            "\n",
            "ğŸ“Š EVALUATION QUALITY METRICS:\n",
            "------------------------------------------------------------\n",
            "âœ… Context Relevance: 0.990 - Excellent\n",
            "âœ… Context Precision: 0.787 - Good\n",
            "âœ… Answer Relevance: 0.980 - Excellent\n",
            "âœ… Faithfulness: 0.925 - Excellent (minimal hallucination)\n",
            "âœ… Correctness: 0.923 - Excellent\n",
            "âœ… Lexical Overlap: 0.723 - Excellent (high word overlap)\n",
            "\n",
            "ğŸ¯ RETRIEVAL PERFORMANCE METRICS:\n",
            "------------------------------------------------------------\n",
            "âœ… Recall@3: 0.943 - Good (capturing most relevant information)\n",
            "âš ï¸  Precision@3: 0.279 - Poor (too much noise in retrieval)\n",
            "âœ… MRR: 0.925 - Good (good ranking quality)\n",
            "âœ… nDCG@3: 0.939 - Good (good overall ranking)\n",
            "\n",
            "ğŸ’¡ OVERALL SYSTEM ANALYSIS:\n",
            "------------------------------------------------------------\n",
            "\n",
            "Overall Quality Score: 0.921\n",
            "\n",
            "âœ… Strengths: Answer Relevance, Faithfulness\n",
            "âš ï¸  Bottlenecks: Retrieval Precision\n",
            "\n",
            "ğŸ“‹ RECOMMENDATIONS:\n",
            "------------------------------------------------------------\n",
            "2. Improve Precision: Add relevance filtering or reduce chunk size\n",
            "\n",
            "============================================================\n",
            "\n",
            "Generating visualizations...\n",
            "âœ… Saved visualizations to: results/metrics_distributions.png\n",
            "\n",
            "============================================================\n",
            "COMPLETE! All metrics calculated and visualized.\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ==================== CONFIGURATION ====================\n",
        "INPUT_FILE = \"results/eval_results.csv\"\n",
        "OUTPUT_FILE = \"results/eval_results_with_retrieval_metrics.csv\"\n",
        "K = 3  # Top K contexts to consider\n",
        "\n",
        "# ==================== LOAD DATA ====================\n",
        "print(\"Loading evaluation results...\")\n",
        "df = pd.read_csv(INPUT_FILE)\n",
        "print(f\"Loaded {len(df)} samples\\n\")\n",
        "\n",
        "# ==================== RETRIEVAL METRICS FUNCTIONS ====================\n",
        "\n",
        "def calculate_recall_at_k(retrieved_contexts, reference_answer, k=3):\n",
        "    \"\"\"Recall@K: What proportion of relevant information was retrieved?\"\"\"\n",
        "    if not retrieved_contexts or not reference_answer:\n",
        "        return 0.0\n",
        "\n",
        "    ref_tokens = set(str(reference_answer).split())\n",
        "    contexts_list = str(retrieved_contexts).split(' | ')[:k]\n",
        "\n",
        "    context_tokens = set()\n",
        "    for ctx in contexts_list:\n",
        "        context_tokens.update(ctx.split())\n",
        "\n",
        "    if len(ref_tokens) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    relevant_found = len(ref_tokens & context_tokens)\n",
        "    return relevant_found / len(ref_tokens)\n",
        "\n",
        "\n",
        "def calculate_precision_at_k(retrieved_contexts, reference_answer, k=3):\n",
        "    \"\"\"Precision@K: What proportion of retrieved information is relevant?\"\"\"\n",
        "    if not retrieved_contexts or not reference_answer:\n",
        "        return 0.0\n",
        "\n",
        "    ref_tokens = set(str(reference_answer).split())\n",
        "    contexts_list = str(retrieved_contexts).split(' | ')[:k]\n",
        "\n",
        "    context_tokens = set()\n",
        "    for ctx in contexts_list:\n",
        "        context_tokens.update(ctx.split())\n",
        "\n",
        "    if len(context_tokens) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    relevant_found = len(ref_tokens & context_tokens)\n",
        "    return relevant_found / len(context_tokens)\n",
        "\n",
        "\n",
        "def calculate_mrr(retrieved_contexts, reference_answer):\n",
        "    \"\"\"MRR: Mean Reciprocal Rank - Position of first relevant context\"\"\"\n",
        "    if not retrieved_contexts or not reference_answer:\n",
        "        return 0.0\n",
        "\n",
        "    ref_tokens = set(str(reference_answer).split())\n",
        "    contexts_list = str(retrieved_contexts).split(' | ')\n",
        "\n",
        "    for rank, ctx in enumerate(contexts_list, start=1):\n",
        "        ctx_tokens = set(ctx.split())\n",
        "        overlap = len(ref_tokens & ctx_tokens)\n",
        "        if overlap > len(ref_tokens) * 0.3:  # 30% overlap threshold\n",
        "            return 1.0 / rank\n",
        "\n",
        "    return 0.0\n",
        "\n",
        "\n",
        "def calculate_ndcg_at_k(retrieved_contexts, reference_answer, k=3):\n",
        "    \"\"\"nDCG@K: Normalized Discounted Cumulative Gain\"\"\"\n",
        "    if not retrieved_contexts or not reference_answer:\n",
        "        return 0.0\n",
        "\n",
        "    ref_tokens = set(str(reference_answer).split())\n",
        "    contexts_list = str(retrieved_contexts).split(' | ')[:k]\n",
        "\n",
        "    # Calculate relevance scores for each context (0-3 scale)\n",
        "    relevance_scores = []\n",
        "    for ctx in contexts_list:\n",
        "        ctx_tokens = set(ctx.split())\n",
        "        overlap = len(ref_tokens & ctx_tokens) / len(ref_tokens) if ref_tokens else 0\n",
        "\n",
        "        if overlap > 0.7:\n",
        "            relevance_scores.append(3)\n",
        "        elif overlap > 0.4:\n",
        "            relevance_scores.append(2)\n",
        "        elif overlap > 0.2:\n",
        "            relevance_scores.append(1)\n",
        "        else:\n",
        "            relevance_scores.append(0)\n",
        "\n",
        "    if not relevance_scores:\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate DCG\n",
        "    dcg = relevance_scores[0]\n",
        "    for i, score in enumerate(relevance_scores[1:], start=2):\n",
        "        dcg += score / np.log2(i + 1)\n",
        "\n",
        "    # Calculate ideal DCG\n",
        "    ideal_scores = sorted(relevance_scores, reverse=True)\n",
        "    idcg = ideal_scores[0]\n",
        "    for i, score in enumerate(ideal_scores[1:], start=2):\n",
        "        idcg += score / np.log2(i + 1)\n",
        "\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "# ==================== CALCULATE METRICS ====================\n",
        "\n",
        "print(\"Calculating retrieval metrics...\")\n",
        "\n",
        "df['recall_at_3'] = df.apply(\n",
        "    lambda row: calculate_recall_at_k(row['retrieved_contexts'], row['reference'], k=K),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "df['precision_at_3'] = df.apply(\n",
        "    lambda row: calculate_precision_at_k(row['retrieved_contexts'], row['reference'], k=K),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "df['mrr'] = df.apply(\n",
        "    lambda row: calculate_mrr(row['retrieved_contexts'], row['reference']),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "df['ndcg_at_3'] = df.apply(\n",
        "    lambda row: calculate_ndcg_at_k(row['retrieved_contexts'], row['reference'], k=K),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(\"Done!\\n\")\n",
        "\n",
        "# ==================== SAVE RESULTS ====================\n",
        "\n",
        "df.to_csv(OUTPUT_FILE, index=False)\n",
        "print(f\"âœ… Saved results to: {OUTPUT_FILE}\\n\")\n",
        "\n",
        "# ==================== DISPLAY SUMMARY ====================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EVALUATION METRICS SUMMARY (Scale 0-1)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "eval_metrics = ['context_relevance', 'context_precision', 'answer_relevance',\n",
        "                'faithfulness', 'correctness', 'lexical_overlap']\n",
        "\n",
        "print(f\"\\n{'Metric':<30} {'Mean':<15} {'Std':<15}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for metric in eval_metrics:\n",
        "    if metric in df.columns:\n",
        "        # Normalize to 0-1 scale (divide by 3 for metrics on 0-3 scale)\n",
        "        if metric == 'lexical_overlap':\n",
        "            # Already on 0-1 scale\n",
        "            mean_val = df[metric].mean()\n",
        "            std_val = df[metric].std()\n",
        "        else:\n",
        "            # Scale from 0-3 to 0-1\n",
        "            mean_val = df[metric].mean() / 3.0\n",
        "            std_val = df[metric].std() / 3.0\n",
        "\n",
        "        print(f\"{metric:<30} {mean_val:<15.3f} {std_val:<15.3f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"RETRIEVAL METRICS SUMMARY (Scale 0-1)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "retrieval_metrics = ['recall_at_3', 'precision_at_3', 'mrr', 'ndcg_at_3']\n",
        "\n",
        "print(f\"\\n{'Metric':<30} {'Mean':<15} {'Std':<15}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for metric in retrieval_metrics:\n",
        "    print(f\"{metric:<30} \"\n",
        "          f\"{df[metric].mean():<15.3f} \"\n",
        "          f\"{df[metric].std():<15.3f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"INTERPRETATION & INSIGHTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Evaluation Metrics Interpretation\n",
        "print(\"\\nğŸ“Š EVALUATION QUALITY METRICS:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "if 'context_relevance' in df.columns:\n",
        "    avg_ctx_rel = df['context_relevance'].mean() / 3.0  # Normalize to 0-1\n",
        "    if avg_ctx_rel >= 0.83:\n",
        "        print(f\"âœ… Context Relevance: {avg_ctx_rel:.3f} - Excellent\")\n",
        "    elif avg_ctx_rel >= 0.67:\n",
        "        print(f\"âœ… Context Relevance: {avg_ctx_rel:.3f} - Good\")\n",
        "    elif avg_ctx_rel >= 0.50:\n",
        "        print(f\"ğŸŸ¡ Context Relevance: {avg_ctx_rel:.3f} - Fair\")\n",
        "    else:\n",
        "        print(f\"âš ï¸  Context Relevance: {avg_ctx_rel:.3f} - Poor\")\n",
        "\n",
        "if 'context_precision' in df.columns:\n",
        "    avg_ctx_prec = df['context_precision'].mean() / 3.0\n",
        "    if avg_ctx_prec >= 0.83:\n",
        "        print(f\"âœ… Context Precision: {avg_ctx_prec:.3f} - Excellent\")\n",
        "    elif avg_ctx_prec >= 0.67:\n",
        "        print(f\"âœ… Context Precision: {avg_ctx_prec:.3f} - Good\")\n",
        "    elif avg_ctx_prec >= 0.50:\n",
        "        print(f\"ğŸŸ¡ Context Precision: {avg_ctx_prec:.3f} - Fair\")\n",
        "    else:\n",
        "        print(f\"âš ï¸  Context Precision: {avg_ctx_prec:.3f} - Poor\")\n",
        "\n",
        "if 'answer_relevance' in df.columns:\n",
        "    avg_ans_rel = df['answer_relevance'].mean() / 3.0\n",
        "    if avg_ans_rel >= 0.83:\n",
        "        print(f\"âœ… Answer Relevance: {avg_ans_rel:.3f} - Excellent\")\n",
        "    elif avg_ans_rel >= 0.67:\n",
        "        print(f\"âœ… Answer Relevance: {avg_ans_rel:.3f} - Good\")\n",
        "    elif avg_ans_rel >= 0.50:\n",
        "        print(f\"ğŸŸ¡ Answer Relevance: {avg_ans_rel:.3f} - Fair\")\n",
        "    else:\n",
        "        print(f\"âš ï¸  Answer Relevance: {avg_ans_rel:.3f} - Poor\")\n",
        "\n",
        "if 'faithfulness' in df.columns:\n",
        "    avg_faith = df['faithfulness'].mean() / 3.0\n",
        "    if avg_faith >= 0.83:\n",
        "        print(f\"âœ… Faithfulness: {avg_faith:.3f} - Excellent (minimal hallucination)\")\n",
        "    elif avg_faith >= 0.67:\n",
        "        print(f\"âœ… Faithfulness: {avg_faith:.3f} - Good\")\n",
        "    elif avg_faith >= 0.50:\n",
        "        print(f\"ğŸŸ¡ Faithfulness: {avg_faith:.3f} - Fair (some hallucination)\")\n",
        "    else:\n",
        "        print(f\"âš ï¸  Faithfulness: {avg_faith:.3f} - Poor (significant hallucination)\")\n",
        "\n",
        "if 'correctness' in df.columns:\n",
        "    avg_correct = df['correctness'].mean() / 3.0\n",
        "    if avg_correct >= 0.83:\n",
        "        print(f\"âœ… Correctness: {avg_correct:.3f} - Excellent\")\n",
        "    elif avg_correct >= 0.67:\n",
        "        print(f\"âœ… Correctness: {avg_correct:.3f} - Good\")\n",
        "    elif avg_correct >= 0.50:\n",
        "        print(f\"ğŸŸ¡ Correctness: {avg_correct:.3f} - Fair\")\n",
        "    else:\n",
        "        print(f\"âš ï¸  Correctness: {avg_correct:.3f} - Poor\")\n",
        "\n",
        "if 'lexical_overlap' in df.columns:\n",
        "    avg_lex = df['lexical_overlap'].mean()\n",
        "    if avg_lex >= 0.7:\n",
        "        print(f\"âœ… Lexical Overlap: {avg_lex:.3f} - Excellent (high word overlap)\")\n",
        "    elif avg_lex >= 0.5:\n",
        "        print(f\"âœ… Lexical Overlap: {avg_lex:.3f} - Good\")\n",
        "    elif avg_lex >= 0.3:\n",
        "        print(f\"ğŸŸ¡ Lexical Overlap: {avg_lex:.3f} - Fair\")\n",
        "    else:\n",
        "        print(f\"âš ï¸  Lexical Overlap: {avg_lex:.3f} - Poor (very different wording)\")\n",
        "\n",
        "# Retrieval Metrics Interpretation\n",
        "print(\"\\nğŸ¯ RETRIEVAL PERFORMANCE METRICS:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "avg_recall = df['recall_at_3'].mean()\n",
        "avg_precision = df['precision_at_3'].mean()\n",
        "avg_mrr = df['mrr'].mean()\n",
        "avg_ndcg = df['ndcg_at_3'].mean()\n",
        "\n",
        "if avg_recall < 0.3:\n",
        "    print(f\"âš ï¸  Recall@3: {avg_recall:.3f} - Poor (missing most relevant information)\")\n",
        "elif avg_recall < 0.5:\n",
        "    print(f\"ğŸŸ¡ Recall@3: {avg_recall:.3f} - Fair (missing some relevant information)\")\n",
        "else:\n",
        "    print(f\"âœ… Recall@3: {avg_recall:.3f} - Good (capturing most relevant information)\")\n",
        "\n",
        "if avg_precision < 0.3:\n",
        "    print(f\"âš ï¸  Precision@3: {avg_precision:.3f} - Poor (too much noise in retrieval)\")\n",
        "elif avg_precision < 0.5:\n",
        "    print(f\"ğŸŸ¡ Precision@3: {avg_precision:.3f} - Fair (moderate noise)\")\n",
        "else:\n",
        "    print(f\"âœ… Precision@3: {avg_precision:.3f} - Good (minimal noise)\")\n",
        "\n",
        "if avg_mrr < 0.4:\n",
        "    print(f\"âš ï¸  MRR: {avg_mrr:.3f} - Poor (relevant docs not ranked well)\")\n",
        "elif avg_mrr < 0.6:\n",
        "    print(f\"ğŸŸ¡ MRR: {avg_mrr:.3f} - Fair (moderate ranking quality)\")\n",
        "else:\n",
        "    print(f\"âœ… MRR: {avg_mrr:.3f} - Good (good ranking quality)\")\n",
        "\n",
        "if avg_ndcg < 0.4:\n",
        "    print(f\"âš ï¸  nDCG@3: {avg_ndcg:.3f} - Poor (poor overall ranking)\")\n",
        "elif avg_ndcg < 0.6:\n",
        "    print(f\"ğŸŸ¡ nDCG@3: {avg_ndcg:.3f} - Fair (moderate overall ranking)\")\n",
        "else:\n",
        "    print(f\"âœ… nDCG@3: {avg_ndcg:.3f} - Good (good overall ranking)\")\n",
        "\n",
        "# Overall System Analysis\n",
        "print(\"\\nğŸ’¡ OVERALL SYSTEM ANALYSIS:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Calculate overall score if final_score exists\n",
        "if 'final_score' in df.columns:\n",
        "    overall_score = df['final_score'].mean() / 3.0  # Normalize to 0-1\n",
        "    print(f\"\\nOverall Quality Score: {overall_score:.3f}\")\n",
        "\n",
        "# Identify main bottleneck\n",
        "bottlenecks = []\n",
        "strengths = []\n",
        "\n",
        "if 'context_relevance' in df.columns and df['context_relevance'].mean() / 3.0 < 0.67:\n",
        "    bottlenecks.append(\"Context Relevance\")\n",
        "if 'answer_relevance' in df.columns and df['answer_relevance'].mean() / 3.0 >= 0.83:\n",
        "    strengths.append(\"Answer Relevance\")\n",
        "if 'faithfulness' in df.columns and df['faithfulness'].mean() / 3.0 >= 0.83:\n",
        "    strengths.append(\"Faithfulness\")\n",
        "if avg_recall < 0.4:\n",
        "    bottlenecks.append(\"Retrieval Recall\")\n",
        "if avg_precision < 0.4:\n",
        "    bottlenecks.append(\"Retrieval Precision\")\n",
        "\n",
        "if strengths:\n",
        "    print(f\"\\nâœ… Strengths: {', '.join(strengths)}\")\n",
        "if bottlenecks:\n",
        "    print(f\"âš ï¸  Bottlenecks: {', '.join(bottlenecks)}\")\n",
        "\n",
        "# Actionable recommendations\n",
        "print(\"\\nğŸ“‹ RECOMMENDATIONS:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "if avg_recall < 0.5:\n",
        "    print(\"1. Improve Recall: Increase number of retrieved documents (n_results)\")\n",
        "if avg_precision < 0.5:\n",
        "    print(\"2. Improve Precision: Add relevance filtering or reduce chunk size\")\n",
        "if avg_mrr < 0.5:\n",
        "    print(\"3. Improve Ranking: Implement reranking (e.g., CrossEncoder)\")\n",
        "if 'correctness' in df.columns and df['correctness'].mean() / 3.0 < 0.67:\n",
        "    print(\"4. Improve Correctness: Better align model outputs with references\")\n",
        "if 'faithfulness' in df.columns and df['faithfulness'].mean() / 3.0 < 0.83:\n",
        "    print(\"5. Reduce Hallucination: Strengthen context grounding in prompts\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# ==================== VISUALIZATIONS ====================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"\\nGenerating visualizations...\")\n",
        "\n",
        "# Create figure with subplots\n",
        "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
        "fig.suptitle('RAG Evaluation Metrics Distribution', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Flatten axes for easier iteration\n",
        "axes = axes.flatten()\n",
        "\n",
        "# All metrics to plot\n",
        "all_metrics = eval_metrics + retrieval_metrics\n",
        "\n",
        "# Plot each metric\n",
        "for idx, metric in enumerate(all_metrics):\n",
        "    if metric in df.columns:\n",
        "        ax = axes[idx]\n",
        "\n",
        "        # Normalize evaluation metrics to 0-1 for consistency\n",
        "        if metric in eval_metrics and metric != 'lexical_overlap':\n",
        "            data = df[metric] / 3.0\n",
        "            scale_label = \"(0-1)\"\n",
        "        else:\n",
        "            data = df[metric]\n",
        "            scale_label = \"(0-1)\"\n",
        "\n",
        "        # Create histogram\n",
        "        ax.hist(data, bins=20, color='steelblue', edgecolor='black', alpha=0.7)\n",
        "\n",
        "        # Add mean line\n",
        "        mean_val = data.mean()\n",
        "        ax.axvline(mean_val, color='red', linestyle='--', linewidth=2,\n",
        "                   label=f'Mean: {mean_val:.3f}')\n",
        "\n",
        "        # Formatting\n",
        "        ax.set_title(metric.replace('_', ' ').title(), fontweight='bold', fontsize=10)\n",
        "        ax.set_xlabel(f'Score {scale_label}', fontsize=9)\n",
        "        ax.set_ylabel('Frequency', fontsize=9)\n",
        "        ax.legend(fontsize=8)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Remove empty subplots if any\n",
        "for idx in range(len(all_metrics), len(axes)):\n",
        "    fig.delaxes(axes[idx])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/metrics_distributions.png', dpi=300, bbox_inches='tight')\n",
        "print(\"âœ… Saved visualizations to: results/metrics_distributions.png\")\n",
        "\n",
        "plt.close()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"COMPLETE! All metrics calculated and visualized.\")\n",
        "print(\"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rag-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
